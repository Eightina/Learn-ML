{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据挖掘电商产品类别预测\n",
    "\n",
    "业务背景为某电商内部上品分类体系融合时，需要对商品进行重新分类。\n",
    "\n",
    "- 评价指标为准确率Accuracy \n",
    "- 训练集train.csv中包含匿名特征及标签target\n",
    "- 测试集test.csv仅包含匿名特征\n",
    "- 项目任务\n",
    "  1. 数据分析可视化，数据处理；\n",
    "  2. 采用lightGBM完成建模；\n",
    "  3. 采用Optuna库对lightGBM模型进行参数优化；\n",
    "  4. 采用PyTorch完成深度学习建模；\n",
    "  5. 融合机器学习 & 深度学习模型预测结果；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#  Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(train.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81819</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52877</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>76874</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Class_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23007</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31906</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>96783</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>51898</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>1492</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>90984</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>12307</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0      81819          0          0          0          0          0   \n",
       "1      52877          1          0          0          2          0   \n",
       "2      76874          0          0          0          0          0   \n",
       "3      23007          0          1          0          0          0   \n",
       "4      31906          0          0          0         11          0   \n",
       "...      ...        ...        ...        ...        ...        ...   \n",
       "79995  96783          0          0          0          4          0   \n",
       "79996  51898          0          0          0          0          0   \n",
       "79997   1492          0          0          0          0          0   \n",
       "79998  90984          5          0          0          0          0   \n",
       "79999  12307          0          0          0          0          0   \n",
       "\n",
       "       feature_5  feature_6  feature_7  feature_8  ...  feature_41  \\\n",
       "0              0          0          3          0  ...           0   \n",
       "1              0          0          0          0  ...           0   \n",
       "2              0          1          1          0  ...           0   \n",
       "3              0          0          0          0  ...           0   \n",
       "4              0          0          5          0  ...           1   \n",
       "...          ...        ...        ...        ...  ...         ...   \n",
       "79995          0          2          0         20  ...           0   \n",
       "79996          0          0          0          0  ...           2   \n",
       "79997          0          0          0          0  ...           0   \n",
       "79998          0         11          2          2  ...           0   \n",
       "79999          0          0          0          0  ...           6   \n",
       "\n",
       "       feature_42  feature_43  feature_44  feature_45  feature_46  feature_47  \\\n",
       "0               0           0           0           1           0           0   \n",
       "1               0           0           0           0           0           2   \n",
       "2               0           0           0           0           0           5   \n",
       "3               0           0           1           0           0           0   \n",
       "4               0           0           0           0           0           0   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "79995           0           0           0           0           0           0   \n",
       "79996           0           0           0           0           0           1   \n",
       "79997           3           0           0           0           0           0   \n",
       "79998           0           0           0           0           0           0   \n",
       "79999           0           0           0           0           0           0   \n",
       "\n",
       "       feature_48  feature_49   target  \n",
       "0               0           0  Class_1  \n",
       "1               1           0  Class_3  \n",
       "2               2           2  Class_2  \n",
       "3               0           0  Class_2  \n",
       "4               2           0  Class_2  \n",
       "...           ...         ...      ...  \n",
       "79995           0           0  Class_4  \n",
       "79996           1           0  Class_2  \n",
       "79997           2           0  Class_4  \n",
       "79998           0           0  Class_2  \n",
       "79999           0           0  Class_2  \n",
       "\n",
       "[80000 rows x 52 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.00000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "      <td>80000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.255738</td>\n",
       "      <td>0.433625</td>\n",
       "      <td>0.113787</td>\n",
       "      <td>0.593300</td>\n",
       "      <td>0.598075</td>\n",
       "      <td>0.16025</td>\n",
       "      <td>0.729513</td>\n",
       "      <td>1.232650</td>\n",
       "      <td>0.911013</td>\n",
       "      <td>0.936600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712087</td>\n",
       "      <td>0.581500</td>\n",
       "      <td>0.525188</td>\n",
       "      <td>0.615012</td>\n",
       "      <td>0.136300</td>\n",
       "      <td>0.357588</td>\n",
       "      <td>0.516088</td>\n",
       "      <td>0.394263</td>\n",
       "      <td>0.972800</td>\n",
       "      <td>0.558187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.922590</td>\n",
       "      <td>1.995876</td>\n",
       "      <td>0.519609</td>\n",
       "      <td>1.856378</td>\n",
       "      <td>2.781237</td>\n",
       "      <td>0.59904</td>\n",
       "      <td>2.334581</td>\n",
       "      <td>2.704171</td>\n",
       "      <td>3.437385</td>\n",
       "      <td>1.898006</td>\n",
       "      <td>...</td>\n",
       "      <td>1.718475</td>\n",
       "      <td>1.995566</td>\n",
       "      <td>2.291033</td>\n",
       "      <td>2.357449</td>\n",
       "      <td>0.631983</td>\n",
       "      <td>1.465016</td>\n",
       "      <td>2.163383</td>\n",
       "      <td>1.504526</td>\n",
       "      <td>2.571059</td>\n",
       "      <td>1.684510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          feature_0     feature_1     feature_2     feature_3     feature_4  \\\n",
       "count  80000.000000  80000.000000  80000.000000  80000.000000  80000.000000   \n",
       "mean       0.255738      0.433625      0.113787      0.593300      0.598075   \n",
       "std        0.922590      1.995876      0.519609      1.856378      2.781237   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       10.000000     31.000000      6.000000     26.000000     38.000000   \n",
       "\n",
       "         feature_5     feature_6     feature_7     feature_8     feature_9  \\\n",
       "count  80000.00000  80000.000000  80000.000000  80000.000000  80000.000000   \n",
       "mean       0.16025      0.729513      1.232650      0.911013      0.936600   \n",
       "std        0.59904      2.334581      2.704171      3.437385      1.898006   \n",
       "min        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.00000      0.000000      1.000000      0.000000      1.000000   \n",
       "max       10.00000     27.000000     30.000000     39.000000     17.000000   \n",
       "\n",
       "       ...    feature_40    feature_41    feature_42    feature_43  \\\n",
       "count  ...  80000.000000  80000.000000  80000.000000  80000.000000   \n",
       "mean   ...      0.712087      0.581500      0.525188      0.615012   \n",
       "std    ...      1.718475      1.995566      2.291033      2.357449   \n",
       "min    ...      0.000000      0.000000     -2.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      1.000000      0.000000      0.000000      0.000000   \n",
       "max    ...     21.000000     32.000000     37.000000     33.000000   \n",
       "\n",
       "         feature_44    feature_45    feature_46    feature_47    feature_48  \\\n",
       "count  80000.000000  80000.000000  80000.000000  80000.000000  80000.000000   \n",
       "mean       0.136300      0.357588      0.516088      0.394263      0.972800   \n",
       "std        0.631983      1.465016      2.163383      1.504526      2.571059   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "max        9.000000     26.000000     28.000000     25.000000     43.000000   \n",
       "\n",
       "         feature_49  \n",
       "count  80000.000000  \n",
       "mean       0.558187  \n",
       "std        1.684510  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max       20.000000  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = train['target']\n",
    "train.drop(['target','id'], axis=1, inplace=True)\n",
    "test.drop(['id'], axis=1, inplace=True)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_sse=[]\n",
    "# for i in range(1,51):\n",
    "#     km = KMeans(n_clusters=i,verbose=-1,\n",
    "#                 random_state=42,n_jobs=-1)\n",
    "#     total_sse.append(km.fit(train).inertia_)\n",
    "# plt.plot(range(1,51),total_sse)\n",
    "# plt.xticks(range(1,51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.00000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.26620</td>\n",
       "      <td>0.424100</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>0.579550</td>\n",
       "      <td>0.606550</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>0.739400</td>\n",
       "      <td>1.214000</td>\n",
       "      <td>0.872700</td>\n",
       "      <td>0.954200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713000</td>\n",
       "      <td>0.584350</td>\n",
       "      <td>0.545400</td>\n",
       "      <td>0.621500</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.362950</td>\n",
       "      <td>0.519700</td>\n",
       "      <td>0.373150</td>\n",
       "      <td>0.963050</td>\n",
       "      <td>0.552850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.95435</td>\n",
       "      <td>1.904132</td>\n",
       "      <td>0.519493</td>\n",
       "      <td>1.796506</td>\n",
       "      <td>2.802702</td>\n",
       "      <td>0.609522</td>\n",
       "      <td>2.378712</td>\n",
       "      <td>2.646498</td>\n",
       "      <td>3.325184</td>\n",
       "      <td>1.928623</td>\n",
       "      <td>...</td>\n",
       "      <td>1.735391</td>\n",
       "      <td>2.033074</td>\n",
       "      <td>2.339575</td>\n",
       "      <td>2.374978</td>\n",
       "      <td>0.609705</td>\n",
       "      <td>1.460898</td>\n",
       "      <td>2.203305</td>\n",
       "      <td>1.416478</td>\n",
       "      <td>2.598773</td>\n",
       "      <td>1.666567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.00000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0     feature_1     feature_2     feature_3     feature_4  \\\n",
       "count  20000.00000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
       "mean       0.26620      0.424100      0.115500      0.579550      0.606550   \n",
       "std        0.95435      1.904132      0.519493      1.796506      2.802702   \n",
       "min        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       10.00000     28.000000      6.000000     26.000000     34.000000   \n",
       "\n",
       "          feature_5     feature_6     feature_7     feature_8     feature_9  \\\n",
       "count  20000.000000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
       "mean       0.163100      0.739400      1.214000      0.872700      0.954200   \n",
       "std        0.609522      2.378712      2.646498      3.325184      1.928623   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      1.000000      0.000000      1.000000   \n",
       "max        8.000000     26.000000     31.000000     32.000000     16.000000   \n",
       "\n",
       "       ...    feature_40    feature_41    feature_42    feature_43  \\\n",
       "count  ...  20000.000000  20000.000000  20000.000000  20000.000000   \n",
       "mean   ...      0.713000      0.584350      0.545400      0.621500   \n",
       "std    ...      1.735391      2.033074      2.339575      2.374978   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      1.000000      0.000000      0.000000      0.000000   \n",
       "max    ...     20.000000     28.000000     34.000000     31.000000   \n",
       "\n",
       "         feature_44    feature_45    feature_46    feature_47    feature_48  \\\n",
       "count  20000.000000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
       "mean       0.130300      0.362950      0.519700      0.373150      0.963050   \n",
       "std        0.609705      1.460898      2.203305      1.416478      2.598773   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      1.000000   \n",
       "max        9.000000     26.000000     29.000000     25.000000     44.000000   \n",
       "\n",
       "         feature_49  \n",
       "count  20000.000000  \n",
       "mean       0.552850  \n",
       "std        1.666567  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max       18.000000  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum().sum(),test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       80000\n",
       "unique          4\n",
       "top       Class_2\n",
       "freq        45962\n",
       "Name: target, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='Count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEHCAYAAACEKcAKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUC0lEQVR4nO3df7DldX3f8eeLXQWsAUEWgruLS2STCTKKshLENNNIUzeTVEiEuKmVnZa6VrGTmNYUQtuM09lWJ5lgtYJuxbJSK2xQC5pQpSAhqQguyG+krEHZdQmgEsRaiAvv/nE+N5693N29y+eee+5hn4+Z75zveZ/v5/v9fD+z8LrfH+d7UlVIkvRs7TfuDkiSJptBIknqYpBIkroYJJKkLgaJJKnL4nF3YL4ddthhtWLFinF3Q5Imys033/ydqloy02f7XJCsWLGCzZs3j7sbkjRRknxrV595akuS1MUgkSR1MUgkSV0MEklSF4NEktTFIJEkdTFIJEldDBJJUheDRJLUZZ/7Zru0UC1dfhTbt20ddzcmykuWLefbWx8Ydzf2eQaJtEBs37aVN3/0y+PuxkS57O0nj7sLwlNbkqROBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4jD5Iki5J8Lcnn2/tDk1yd5L72esjQsucm2ZLk3iRvGKqfkOSO9tkHk6TV909yWavfmGTFqPdHkrSz+Tgi+S3gnqH35wDXVNVK4Jr2niTHAmuAlwOrgQuSLGptLgTWASvbtLrVzwIerapjgPOB9492VyRJ0400SJIsA34F+NhQ+VRgY5vfCJw2VL+0qp6sqvuBLcCJSY4EDqqqG6qqgE9MazO1rsuBU6aOViRJ82PURyQfAH4XeHqodkRVPQjQXg9v9aXA1qHltrXa0jY/vb5Tm6raATwGvHhO90CStFsjC5Ikvwo8XFU3z7bJDLXaTX13bab3ZV2SzUk2P/LII7PsjiRpNkZ5RPI64I1JvglcCrw+yX8DHmqnq2ivD7fltwHLh9ovA7a3+rIZ6ju1SbIYOBj43vSOVNWGqlpVVauWLFkyN3snSQJGGCRVdW5VLauqFQwuol9bVf8YuBJY2xZbC1zR5q8E1rQ7sY5mcFH9pnb66/EkJ7XrH2dOazO1rtPbNp5xRCJJGp3FY9jm+4BNSc4CHgDOAKiqu5JsAu4GdgBnV9VTrc07gIuBA4Gr2gRwEXBJki0MjkTWzNdOSJIG5iVIquo64Lo2/13glF0stx5YP0N9M3DcDPUnaEEkSRoPv9kuSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpy8iCJMkBSW5KcluSu5K8t9UPTXJ1kvva6yFDbc5NsiXJvUneMFQ/Ickd7bMPJkmr75/ksla/McmKUe2PJGlmozwieRJ4fVW9EjgeWJ3kJOAc4JqqWglc096T5FhgDfByYDVwQZJFbV0XAuuAlW1a3epnAY9W1THA+cD7R7g/kqQZjCxIauAH7e3z2lTAqcDGVt8InNbmTwUuraonq+p+YAtwYpIjgYOq6oaqKuAT09pMrety4JSpoxVJ0vwY6TWSJIuS3Ao8DFxdVTcCR1TVgwDt9fC2+FJg61Dzba22tM1Pr+/Upqp2AI8BLx7JzkiSZjTSIKmqp6rqeGAZg6OL43az+ExHErWb+u7a7LziZF2SzUk2P/LII3votSRpb8zLXVtV9dfAdQyubTzUTlfRXh9ui20Dlg81WwZsb/VlM9R3apNkMXAw8L0Ztr+hqlZV1aolS5bMzU5JkoDR3rW1JMmL2vyBwN8Hvg5cCaxti60FrmjzVwJr2p1YRzO4qH5TO/31eJKT2vWPM6e1mVrX6cC17TqKJGmeLB7huo8ENrY7r/YDNlXV55PcAGxKchbwAHAGQFXdlWQTcDewAzi7qp5q63oHcDFwIHBVmwAuAi5JsoXBkciaEe6PJGkGIwuSqrodeNUM9e8Cp+yizXpg/Qz1zcAzrq9U1RO0IJIkjYffbJckdTFIJEldDBJJUheDRJLUxSCRJHUxSCRJXQwSSVIXg0SS1MUgkSR1mVWQJHndbGqSpH3PbI9IPjTLmiRpH7PbZ20leS1wMrAkye8MfXQQsGjmVpKkfcmeHtr4fOCFbbmfGKp/n8Fj2yVJ+7jdBklV/RnwZ0kurqpvzVOfJEkTZLaPkd8/yQZgxXCbqnr9KDolSZocsw2SPwY+AnwMeGoPy0qS9iGzDZIdVXXhSHsiSZpIs73993NJ3pnkyCSHTk0j7ZkkaSLM9ohkbXt9z1CtgJ+a2+5IkibNrIKkqo4edUckSZNpVkGS5MyZ6lX1ibntjiRp0sz21NZrhuYPAE4BbgEMEknax8321Na/GH6f5GDgkpH0SJI0UZ7tY+R/CKycy45IkibTbK+RfI7BXVoweFjjzwKbRtUpSdLkmO01kj8cmt8BfKuqto2gP5KkCTOrU1vt4Y1fZ/AE4EOAvxllpyRJk2O2v5D4G8BNwBnAbwA3JvEx8pKkWZ/aOg94TVU9DJBkCfC/gMtH1TFJ0mSY7V1b+02FSPPdvWgrSXoOm+0Ryf9M8gXgU+39m4E/HU2XJEmTZE+/2X4McERVvSfJrwM/DwS4AfjkPPRPkrTA7en01AeAxwGq6jNV9TtV9W4GRyMfGG3XJEmTYE9BsqKqbp9erKrNDH52V5K0j9tTkBywm88OnMuOSJIm056C5KtJ3ja9mOQs4ObRdEmSNEn2dNfWbwOfTfIWfhwcq4DnA782wn5JkibEbo9IquqhqjoZeC/wzTa9t6peW1V/tbu2SZYn+VKSe5LcleS3Wv3QJFcnua+9HjLU5twkW5Lcm+QNQ/UTktzRPvtgkrT6/kkua/Ubk6x4luMgSXqWZvusrS9V1YfadO0s170D+JdV9bPAScDZSY4FzgGuqaqVwDXtPe2zNcDLgdXABUkWtXVdCKxj8Oj6le1zgLOAR6vqGOB84P2z7JskaY6M7NvpVfVgVd3S5h8H7gGWAqcCG9tiG4HT2vypwKVV9WRV3Q9sAU5MciRwUFXdUFXF4FcZh9tMrety4JSpoxVJ0vyYl8ectFNOrwJuZPAFxwdhEDbA4W2xpcDWoWbbWm1pm59e36lNVe0AHgNePMP21yXZnGTzI488Mkd7JUmCeQiSJC8EPg38dlV9f3eLzlCr3dR312bnQtWGqlpVVauWLFmypy5LkvbCSIMkyfMYhMgnq+ozrfxQO11Fe516GOQ2YPlQ82XA9lZfNkN9pzZJFgMHA9+b+z2RJO3KyIKkXau4CLinqv5o6KMrgbVtfi1wxVB9TbsT62gGF9Vvaqe/Hk9yUlvnmdPaTK3rdODadh1FkjRPZvv032fjdcBbgTuS3Npqvwe8D9jUvtT4AIMfy6Kq7kqyCbibwR1fZ1fVU63dO4CLGXyb/qo2wSCoLkmyhcGRyJoR7o8kaQYjC5Kq+gtmvoYBcMou2qwH1s9Q3wwcN0P9CVoQSZLGwx+nkiR1MUgkSV0MEklSF4NEktTFIJEkdTFIJEldDBJJUpdRfiFR+7ily49i+7ate15Q0kQzSDQy27dt5c0f/fK4uzExLnv7yePugvSseGpLktTFIJEkdTFIJEldDBJJUheDRJLUxSCRJHUxSCRJXQwSSVIXg0SS1MUgkSR1MUgkSV0MEklSF4NEktTFIJEkdTFIJEldDBJJUheDRJLUxSCRJHUxSCRJXQwSSVIXg0SS1MUgkSR1MUgkSV0MEklSF4NEktTFIJEkdTFIJEldRhYkST6e5OEkdw7VDk1ydZL72ushQ5+dm2RLknuTvGGofkKSO9pnH0ySVt8/yWWtfmOSFaPaF0nSro3yiORiYPW02jnANVW1ErimvSfJscAa4OWtzQVJFrU2FwLrgJVtmlrnWcCjVXUMcD7w/pHtiSRpl0YWJFV1PfC9aeVTgY1tfiNw2lD90qp6sqruB7YAJyY5Ejioqm6oqgI+Ma3N1LouB06ZOlqRJM2fxfO8vSOq6kGAqnowyeGtvhT4ytBy21rtR21+en2qzda2rh1JHgNeDHxn+kaTrGNwVMNRRx01Zzsjacz2W4x/P87eS5Yt59tbH5jz9c53kOzKTP8Sajf13bV5ZrFqA7ABYNWqVTMuI2kCPb2DN3/0y+PuxcS47O0nj2S9833X1kPtdBXt9eFW3wYsH1puGbC91ZfNUN+pTZLFwME881SaJGnE5jtIrgTWtvm1wBVD9TXtTqyjGVxUv6mdBns8yUnt+seZ09pMret04Np2HUWSNI9GdmoryaeAvwcclmQb8PvA+4BNSc4CHgDOAKiqu5JsAu4GdgBnV9VTbVXvYHAH2IHAVW0CuAi4JMkWBkcia0a1L5KkXRtZkFTVb+7io1N2sfx6YP0M9c3AcTPUn6AFkSRpfPxmuySpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgvloY0TYenyo9i+beu4uyFJC4pBshe2b9vqk0b3wqieNCppYfHUliSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSeoy8UGSZHWSe5NsSXLOuPsjSfuaiQ6SJIuADwO/DBwL/GaSY8fbK0nat0x0kAAnAluq6i+r6m+AS4FTx9wnSdqnpKrG3YdnLcnpwOqq+mft/VuBn6uqd01bbh2wrr39GeDeee3o6B0GfGfcnZggjtfec8z2znNxvF5aVUtm+mDxfPdkjmWG2jOSsao2ABtG353xSLK5qlaNux+TwvHae47Z3tnXxmvST21tA5YPvV8GbB9TXyRpnzTpQfJVYGWSo5M8H1gDXDnmPknSPmWiT21V1Y4k7wK+ACwCPl5Vd425W+PwnD1tNyKO195zzPbOPjVeE32xXZI0fpN+akuSNGYGiSSpi0EiSepikIxZkp9McmmSbyS5O8mfJvnpJHeOeLtnJLkrydNJJuZ+9zGO179PcnuSW5N8MclLRrm9uTLG8fqDJF9vY/bZJC8a5fbmyrjGa2j7/ypJJTlsPrY3VwySMUoS4LPAdVX1sqo6Fvg94Ih52PydwK8D18/DtubEmMfrD6rqFVV1PPB54N/Nwza7jHm8rgaOq6pXAP8HOHcettllzONFkuXALwEPzMf25pJBMl6/CPyoqj4yVaiqW4GtU++TrEjy50luadPJrX5kkuvbX8h3Jvm7SRYlubi9vyPJu3e14aq6p6om7VEx4xyv7w+9/TvM8ASFBWic4/XFqtrR3n6FwZeFF7qxjVdzPvC7TMa/rZ1M9PdIngOOA27ewzIPA79UVU8kWQl8ClgF/CPgC1W1vj0F+QXA8cDSqjoOYFJOJ+yFsY5XkvXAmcBjDP6ns9AtlH9f/xS4bO+7P+/GNl5J3gh8u6puGxwYTRaDZOF7HvCfkxwPPAX8dKt/Ffh4kucB/6Oqbk3yl8BPJfkQ8CfAF8fR4TEb2XhV1XnAeUnOBd4F/P6I9mE+jfTfV5LzgB3AJ0fR+TGY8/FK8gLgPOAfjLrzo+KprfG6CzhhD8u8G3gIeCWDv3yeD1BV1wO/AHwbuCTJmVX1aFvuOuBs4GOj6fbYLJTx+u/Am/a282Mw1vFKshb4VeAtNRnffB7XeL0MOBq4Lck3GZwGvCXJT/bszHwySMbrWmD/JG+bKiR5DfDSoWUOBh6sqqeBtzJ4FAxJXgo8XFX/BbgIeHW702O/qvo08G+BV8/PbsybsY1XO40x5Y3A1+dml0ZqnOO1GvjXwBur6odzu1sjM5bxqqo7qurwqlpRVSsYPIz21VX1V3O+h6NSVU5jnICXAJuAbzD4i+hPgJXAne3zlcDtDC5Y/kfgB62+lsGdV18D/pzBXzSvBG4Bbm3TL+9mu7/G4B/skwz+wvrCuMdigY/Xp1v724HPMTj3PfbxWMDjtYXBReqpZT8y7rFYyOM1rQ/fBA4b91jszeSztiRJXTy1JUnq4l1bz3FJPgy8blr5P1XVfx1HfxY6x2vvOF5757k6Xp7akiR18dSWJKmLQSJJ6mKQSHMsyYuSvHMetnNakmNHvR1pTwwSae69CJh1kGTg2fy3eBpgkGjsvNguzbEklwKnAvcCXwJeARzC4DlN/6aqrkiyAriqff5aBqFwJvAWBl/k+w5wc1X9YZKXAR8GlgA/BN4GHMrgcfaPtelNVfWNedpFaSfe/ivNvXMY/BbH8UkWAy+oqu+3R2Z8JcmVbbmfAf5JVb0zgx8XexPwKgb/Xd7Cj59EuwH451V1X5KfAy6oqte39Xy+qi6fz52TpjNIpNEK8B+S/ALwNLCUH/9Q0req6itt/ueBK6rq/wEk+Vx7fSFwMvDHQ48X33+e+i7NikEijdZbGJySOqGqftSe7npA++z/Di23qx+h2A/46xr8MqO0IHmxXZp7jwM/0eYPZvBU2B8l+UV2fpLssL8A/mGSA9pRyK/A3/4y4/1JzoC/vTD/yhm2I42NQSLNsar6LvC/k9zJ4FfyViXZzODoZMbHz1fVV4ErgduAzwCbGVxEp7U7K8ltDJ5Ie2qrXwq8J8nX2gV5aSy8a0taIJK8sKp+0H4x73pgXVXdMu5+SXviNRJp4djQvmB4ALDRENGk8IhEktTFaySSpC4GiSSpi0EiSepikEiSuhgkkqQu/x8NQeDaVHCb9AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_40</th>\n",
       "      <th>feature_41</th>\n",
       "      <th>feature_42</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_44</th>\n",
       "      <th>feature_45</th>\n",
       "      <th>feature_46</th>\n",
       "      <th>feature_47</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.257830</td>\n",
       "      <td>0.431720</td>\n",
       "      <td>0.114130</td>\n",
       "      <td>0.590550</td>\n",
       "      <td>0.599770</td>\n",
       "      <td>0.160820</td>\n",
       "      <td>0.731490</td>\n",
       "      <td>1.228920</td>\n",
       "      <td>0.903350</td>\n",
       "      <td>0.940120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712270</td>\n",
       "      <td>0.582070</td>\n",
       "      <td>0.529230</td>\n",
       "      <td>0.616310</td>\n",
       "      <td>0.135100</td>\n",
       "      <td>0.358660</td>\n",
       "      <td>0.516810</td>\n",
       "      <td>0.39004</td>\n",
       "      <td>0.970850</td>\n",
       "      <td>0.55712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.929033</td>\n",
       "      <td>1.977862</td>\n",
       "      <td>0.519584</td>\n",
       "      <td>1.844558</td>\n",
       "      <td>2.785531</td>\n",
       "      <td>0.601149</td>\n",
       "      <td>2.343465</td>\n",
       "      <td>2.692732</td>\n",
       "      <td>3.415258</td>\n",
       "      <td>1.904172</td>\n",
       "      <td>...</td>\n",
       "      <td>1.721863</td>\n",
       "      <td>2.003114</td>\n",
       "      <td>2.300826</td>\n",
       "      <td>2.360955</td>\n",
       "      <td>0.627592</td>\n",
       "      <td>1.464187</td>\n",
       "      <td>2.171415</td>\n",
       "      <td>1.48735</td>\n",
       "      <td>2.576615</td>\n",
       "      <td>1.68093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>20.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature_0      feature_1      feature_2      feature_3  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.257830       0.431720       0.114130       0.590550   \n",
       "std         0.929033       1.977862       0.519584       1.844558   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max        10.000000      31.000000       6.000000      26.000000   \n",
       "\n",
       "           feature_4      feature_5      feature_6      feature_7  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.599770       0.160820       0.731490       1.228920   \n",
       "std         2.785531       0.601149       2.343465       2.692732   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       1.000000   \n",
       "max        38.000000      10.000000      27.000000      31.000000   \n",
       "\n",
       "           feature_8      feature_9  ...     feature_40     feature_41  \\\n",
       "count  100000.000000  100000.000000  ...  100000.000000  100000.000000   \n",
       "mean        0.903350       0.940120  ...       0.712270       0.582070   \n",
       "std         3.415258       1.904172  ...       1.721863       2.003114   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "50%         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "75%         0.000000       1.000000  ...       1.000000       0.000000   \n",
       "max        39.000000      17.000000  ...      21.000000      32.000000   \n",
       "\n",
       "          feature_42     feature_43     feature_44     feature_45  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.529230       0.616310       0.135100       0.358660   \n",
       "std         2.300826       2.360955       0.627592       1.464187   \n",
       "min        -2.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max        37.000000      33.000000       9.000000      26.000000   \n",
       "\n",
       "          feature_46    feature_47     feature_48    feature_49  \n",
       "count  100000.000000  100000.00000  100000.000000  100000.00000  \n",
       "mean        0.516810       0.39004       0.970850       0.55712  \n",
       "std         2.171415       1.48735       2.576615       1.68093  \n",
       "min         0.000000       0.00000       0.000000       0.00000  \n",
       "25%         0.000000       0.00000       0.000000       0.00000  \n",
       "50%         0.000000       0.00000       0.000000       0.00000  \n",
       "75%         0.000000       0.00000       1.000000       0.00000  \n",
       "max        29.000000      25.00000      44.000000      20.00000  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train,test], axis=0)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_sum = np.array(df).sum(1)\n",
    "# df['sum'] = val_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# train = np.array(scaler.fit_transform(train))\n",
    "# test = np.array(scaler.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(target.map({\"Class_1\":0, \"Class_2\":1, \"Class_3\":2, \"Class_4\":3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# GBM Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params Optinization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    k_fold=3\n",
    "    accuracy_sum = 0\n",
    "    for i in range(k_fold):\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(train, target, test_size=0.35)\n",
    "        lgb_train_targets = lgb.Dataset(train_x, label=train_y)\n",
    "        param = {\n",
    "\n",
    "            \"objective\": \"multiclass\",\n",
    "            \"metric\": \"multi_error\",\n",
    "            \"verbosity\": -1,\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"num_class\":4, \n",
    "            # 'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 0.9),\n",
    "            # 'subsample': trial.suggest_float(\"subsample\", 0.6, 0.8),\n",
    "            'learning_rate': trial.suggest_float(\"learning_rate\", 0.05, 0.2),\n",
    "            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 16, 1024),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "            'seed': 42\n",
    "        }\n",
    "        \n",
    "        gbm = lgb.train(param, lgb_train_targets)\n",
    "        preds = gbm.predict(valid_x)\n",
    "        preds = np.array([np.argwhere(x == x.max())for x in preds]).squeeze()\n",
    "        accuracy_sum += sklearn.metrics.accuracy_score(valid_y, preds)\n",
    "    accuracy = accuracy_sum/k_fold\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Besttrial:\n",
      "Value: 0.5767857142857143\n",
      "Params:\n",
      "learning_rate:0.013031298252275447\n",
      "lambda_l1:9.467436548086597e-08\n",
      "lambda_l2:0.27085918847321766\n",
      "num_leaves:223\n",
      "n_estimators:595\n",
      "feature_fraction:0.4124325977011139\n",
      "bagging_fraction:0.7005601297310573\n",
      "bagging_freq:2\n",
      "min_child_samples:71\n"
     ]
    }
   ],
   "source": [
    "print(\"Besttrial:\")\n",
    "trial=study.best_trial\n",
    "print(\"Value:\",trial.value)\n",
    "print(\"Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}:{}\".format(key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gbmpredict(train=train, target=target):\n",
    "#     train_x, valid_x, train_y, valid_y = train_test_split(train, target, test_size=0.1)\n",
    "#     lgb_train_targets = lgb.Dataset(train_x, label=train_y)\n",
    "#     param = {\n",
    "#         \"objective\": \"multiclass\",\n",
    "#         \"metric\": \"multi_error\",\n",
    "#         \"verbosity\": -1,\n",
    "#         \"boosting_type\": \"gbdt\",\n",
    "#         \"num_class\":4, \n",
    "#                 # 'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 0.9),\n",
    "#                 # 'subsample': trial.suggest_float(\"subsample\", 0.6, 0.8),\n",
    "#         'learning_rate': 0.05969785593242574,\n",
    "#         'lambda_l1': 9.57481277431383,\n",
    "#         'lambda_l2': 0.005569190507851442,\n",
    "#         'feature_fraction': 0.441560863743879,\n",
    "#         'bagging_fraction': 0.5669317908216961, \n",
    "#             'bagging_freq': 2,\n",
    "#             'min_child_samples': 7,\n",
    "#         'learning_rate': 0.05, \n",
    "#         'num_leaves': 39, \n",
    "#         \"n_estimators\":4096,\n",
    "#     }\n",
    "\n",
    "#     # k = 5\n",
    "#     # gbm = lgb.train(param, lgb_train_targets)\n",
    "#     # preds = gbm.predict(valid_x)\n",
    "#     # preds = np.array([np.argwhere(x == x.max())for x in preds]).squeeze()\n",
    "#     # accuracy_sum += sklearn.metrics.accuracy_score(valid_y, preds)\n",
    "#     # accuracy = accuracy_sum/k_fold\n",
    "#     # return accuracy      \n",
    "#     gbm = lgb.train(param, lgb_train_targets)\n",
    "#     lgb.plot_importance(gbm)\n",
    "#     plt.figure(figsize=(8,20),dpi = 100)\n",
    "#     plt.show()\n",
    "#     preds = gbm.predict(train)\n",
    "#     return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation & Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbmpredict(train=train, target=target, test=test):\n",
    "    k_fold=5\n",
    "    accuracy_sum = 0\n",
    "    preds_on_test = 0\n",
    "    preds_on_train = 0\n",
    "    for i in range(k_fold):\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(train, target, test_size=0.2)\n",
    "        lgb_train_targets = lgb.Dataset(train_x, label=train_y)\n",
    "        param = {\n",
    "            \"objective\": \"multiclass\",\n",
    "            \"metric\": \"multi_error\",\n",
    "            \"verbosity\": -1,\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"num_class\":4, \n",
    "                    # 'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.5, 0.9),\n",
    "                    # 'subsample': trial.suggest_float(\"subsample\", 0.6, 0.8),\n",
    "            'lambda_l1': 1.262252332496178,\n",
    "            'lambda_l2': 0.29444165244400033,\n",
    "            'feature_fraction': 0.7171898178218491,\n",
    "            'bagging_fraction': 0.8102191911790236,\n",
    "            'bagging_freq': 5,\n",
    "            'min_child_samples': 28,\n",
    "            'learning_rate': 0.05, \n",
    "            'num_leaves': 39, \n",
    "            \"n_estimators\":4096,\n",
    "\n",
    "        }\n",
    "        # gbm = lgb.train(param, lgb_train_targets)\n",
    "        # preds = gbm.predict(valid_x)\n",
    "        # preds = np.array([np.argwhere(x == x.max())for x in preds]).squeeze()\n",
    "        # print(sklearn.metrics.accuracy_score(valid_y, preds))\n",
    "        \n",
    "        gbm = lgb.train(param, lgb_train_targets)\n",
    "        preds = gbm.predict(valid_x)\n",
    "        preds = np.array([np.argwhere(x == x.max())for x in preds]).squeeze()\n",
    "        print(sklearn.metrics.accuracy_score(valid_y, preds))\n",
    "        \n",
    "        preds_on_test += gbm.predict(test)/k_fold\n",
    "        preds_on_train += gbm.predict(train)/k_fold\n",
    "        \n",
    "    return preds_on_train,preds_on_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5555625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\envs\\python38\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5561875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\envs\\python38\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.561625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\envs\\python38\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5539375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\envs\\python38\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.556625\n"
     ]
    }
   ],
   "source": [
    "preds_on_train,preds_on_test = gbmpredict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_on_train = np.array(preds_on_train)\n",
    "preds_on_test = np.array(preds_on_test)\n",
    "preds_on_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_ba = pd.DataFrame(preds_on_test)\n",
    "gbm_ba.to_csv('gbm_ba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "preds_on_train = np.array([np.argwhere(x == x.max())for x in preds_on_train]).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "sns.histplot(preds_on_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8390625"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.accuracy_score(target, preds_on_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Torch Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "NUM_WORKERS = 0\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features & Loss Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df.drop(['id'], axis=1, inplace=True)\n",
    "    # for i,col in enumerate(df.columns):\n",
    "    #     df[col+str(2)] = df[col]*df[col]\n",
    "    #     df[col+str(3)] = df[col+str(2)]*df[col]\n",
    "        # df[col+'log'] = np.log(df[col])\n",
    "        # df[col+'log'].loc[df[col+'log']==np.inf]=0\n",
    "        # df[col+'log'].loc[df[col+'log']==-np.inf]=0\n",
    "    # km = KMeans(n_clusters=24,random_state=42)\n",
    "    # cluster = km.fit_predict(df)\n",
    "    # df['Cluster'] = cluster \n",
    "    df.fillna(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_weight(train):\n",
    "    col_name = train.columns[3]\n",
    "    class_cnt = np.array(train.groupby('target').agg('count')[col_name])\n",
    "    \n",
    "    entropy_weight = torch.Tensor(np.exp(np.sum(np.log2(class_cnt))/np.log2(class_cnt)))\n",
    "    entropy_weight = entropy_weight.cuda()\n",
    "    return entropy_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x20b944b5dc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def prepare_data(train, test):\n",
    "    target = train['target']\n",
    "    target = np.array(target.map({\"Class_1\":0, \"Class_2\":1, \"Class_3\":2, \"Class_4\":3}))\n",
    "    train.drop('target', axis=1, inplace=True)\n",
    "    \n",
    "    add_features(train)\n",
    "    add_features(test)\n",
    "    scaler = StandardScaler()\n",
    "    train = pd.DataFrame(np.array(scaler.fit_transform(train)))\n",
    "    test = pd.DataFrame(np.array(scaler.transform(test)))\n",
    "    \n",
    "    train['target'] = target\n",
    "    \n",
    "    entropy_weight = calc_weight(train)\n",
    "    \n",
    "    return train, test, test.shape[1], entropy_weight\n",
    "train,test,INPUT_DIM,ENTROPY_WEIGHT = prepare_data(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "if debug:\n",
    "    train = train[:8000]\n",
    "else:\n",
    "    train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BusinessData(Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        if \"target\" not in df.columns:\n",
    "            df['target'] = 0\n",
    "        self.df = df\n",
    "        self.x_data = torch.Tensor(np.array(df.drop('target', axis=1)))\n",
    "        self.y_data = torch.Tensor(np.array(df.target))\n",
    "        self.len = df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = {\n",
    "            \"x_data\":  torch.tensor(self.x_data[index],dtype=torch.float),\n",
    "            \"y_data\": torch.tensor(self.y_data[index],dtype=torch.float)\n",
    "        }\n",
    "        return data \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# business_data = BusinessData()\n",
    "\n",
    "# train_loader = DataLoader(dataset=business_data,\n",
    "#                         batch_size=128,\n",
    "#                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MlpModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=50,\n",
    "        dense_dim=256,\n",
    "        sep_dim1=256,\n",
    "        sep_dim2=256,        \n",
    "        comb_dim1=256,\n",
    "        comb_dim2=256,\n",
    "        last_dim=256,\n",
    "        num_classes=4\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, dense_dim),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(dense_dim, sep_dim1),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(sep_dim1, dense_dim //2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(dense_dim//2, sep_dim1//2),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "        self.mlp3 = nn.Sequential(\n",
    "            nn.Linear(sep_dim1, dense_dim*2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(dense_dim*2, sep_dim1//2),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "        self.mlp4 = nn.Sequential(\n",
    "            nn.Linear(sep_dim1, dense_dim // 2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(dense_dim // 2, num_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        features = self.mlp1(x)\n",
    "        features1 = self.mlp2(features)\n",
    "        features2 = self.mlp3(features)\n",
    "        pred = self.mlp4(torch.cat((features1,features2), dim=1))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "\n",
    "# class MlpModel(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         input_dim=50,\n",
    "#         dense_dim=256,\n",
    "#         sep_dim1=256,\n",
    "#         sep_dim2=256,        \n",
    "#         comb_dim1=256,\n",
    "#         comb_dim2=256,\n",
    "#         last_dim=256,\n",
    "#         num_classes=4,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.mlp1 = nn.Sequential(\n",
    "#             nn.Linear(input_dim, dense_dim),\n",
    "#             nn.SELU(),\n",
    "#             nn.Linear(dense_dim, sep_dim1),\n",
    "#             nn.SELU(),\n",
    "#         )\n",
    "        \n",
    "#         self.mlp2 = nn.Sequential(\n",
    "#             nn.Linear(sep_dim1, dense_dim),\n",
    "#             nn.SELU(),\n",
    "#             nn.Linear(dense_dim, sep_dim2),\n",
    "#             nn.SELU(),\n",
    "#         )\n",
    "        \n",
    "#         self.mlp3 = nn.Sequential(\n",
    "#             nn.Linear(sep_dim1, dense_dim // 2),\n",
    "#             nn.SELU(),\n",
    "#             nn.Linear(dense_dim // 2, sep_dim2),\n",
    "#             nn.SELU(),\n",
    "#         )\n",
    "#         self.mlp4 = nn.Sequential(\n",
    "#             nn.Linear(sep_dim2, dense_dim*2 ),\n",
    "#             nn.SELU(),\n",
    "#             nn.Linear(dense_dim*2 , comb_dim1),\n",
    "#             nn.SELU(),\n",
    "#         )\n",
    "#         self.mlp5 = nn.Sequential(\n",
    "#             nn.Linear(2*sep_dim2, dense_dim),\n",
    "#             nn.SELU(),\n",
    "#             nn.Linear(dense_dim, comb_dim2),\n",
    "#             nn.SELU(),\n",
    "#         )\n",
    "#         self.mlp6= nn.Sequential(\n",
    "#             nn.Linear(comb_dim1+comb_dim2, dense_dim//2),\n",
    "#             nn.SELU(),\n",
    "#             nn.Linear(dense_dim//2, last_dim),\n",
    "#             nn.SELU(),\n",
    "#         )\n",
    "#         self.mlp7 = nn.Sequential(\n",
    "#             nn.Linear(last_dim+sep_dim2, dense_dim // 2),\n",
    "#             nn.SELU(),\n",
    "#             nn.Linear(dense_dim // 2, num_classes),\n",
    "#         )\n",
    "        \n",
    "\n",
    "#     def forward(self, features):\n",
    "#         features = self.mlp1(features)\n",
    "#         features_sep1 = self.mlp2(features)\n",
    "#         features_sep2 = self.mlp3(features)\n",
    "#         features_sep3 = self.mlp4(features_sep1)\n",
    "#         features_sep4 = self.mlp5(torch.cat((features_sep1,features_sep2), dim=1))\n",
    "#         features_sep1 = self.mlp6(torch.cat((features_sep3,features_sep4), dim=1))\n",
    "#         features_sep2 = self.mlp7(torch.cat((features_sep1,features_sep2), dim=1))\n",
    "        \n",
    "#         return features_sep2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(df, preds):\n",
    "    \"\"\"\n",
    "    Metric for the problem, as I understood it.\n",
    "    \"\"\"\n",
    "    \n",
    "    y = np.array(df['target'].values.tolist())\n",
    "    # preds = np.array([np.argwhere(x == x.max())for x in preds]).squeeze()\n",
    "    accuracy = np.array(preds == y).sum()/len(y)\n",
    "    # accuracy = sklearn.metrics.accuracy_score(y, preds)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    loss_name=\"L1Loss\",\n",
    "    optimizer=\"Adam\",\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    val_bs=32,\n",
    "    warmup_prop=0.1,\n",
    "    lr=1e-3,\n",
    "    num_classes=1,\n",
    "    verbose=1,\n",
    "    first_epoch_eval=0,\n",
    "    device=\"cuda\",\n",
    "    entropy_wieght=[1,1,1]\n",
    "):\n",
    "    avg_val_loss = 0.\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = getattr(torch.optim, optimizer)(model.parameters(), lr=lr)\n",
    "\n",
    "    # Data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=val_bs,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # Scheduler\n",
    "    num_warmup_steps = int(warmup_prop * epochs * len(train_loader))\n",
    "    num_training_steps = int(epochs * len(train_loader))\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps, num_training_steps\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        start_time = time.time()\n",
    "\n",
    "        avg_loss = 0\n",
    "        for data in train_loader:\n",
    "            # pred = model(data['x_data'].to(device)).squeeze(-1)\n",
    "            pred = model(data['x_data'].to(device))\n",
    "            criteria = nn.CrossEntropyLoss(weight=entropy_wieght)\n",
    "            loss = criteria(\n",
    "                pred,\n",
    "                data['y_data'].to(device).long()\n",
    "            )\n",
    "            loss.backward()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            for param in model.parameters():\n",
    "                param.grad = None\n",
    "\n",
    "        model.eval()\n",
    "        accuracy, avg_val_loss = 0, 0\n",
    "        preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                # pred = model(data['x_data'].to(device)).squeeze(-1)\n",
    "                pred = model(data['x_data'].to(device))\n",
    "                criteria = nn.CrossEntropyLoss(weight=entropy_wieght)\n",
    "                loss = criteria(\n",
    "                    pred.detach(),\n",
    "                    data['y_data'].to(device).long()\n",
    "                )\n",
    "                avg_val_loss += loss.item() / len(val_loader)\n",
    "                \n",
    "                pred = np.array([np.argwhere(x == x.max())for x in pred.detach().cpu().numpy()]).squeeze()\n",
    "                preds.append(pred)\n",
    "        \n",
    "        preds = np.concatenate(preds, 0)\n",
    "        accuracy = compute_metric(val_dataset.df, preds)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if (epoch + 1) % verbose == 0:\n",
    "            elapsed_time = elapsed_time * verbose\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1:02d}/{epochs:02d} \\t lr={lr:.1e}\\t t={elapsed_time:.0f}s \\t\"\n",
    "                f\"loss={avg_loss:.3f}\",\n",
    "                end=\"\\t\",\n",
    "            )\n",
    "\n",
    "            if (epoch + 1 >= first_epoch_eval) or (epoch + 1 == epochs):\n",
    "                print(f\"val_loss={avg_val_loss:.3f}\\taccuray={accuracy:.3f}\")\n",
    "            else:\n",
    "                print(\"\")\n",
    "\n",
    "    del (val_loader, train_loader, loss, data, pred)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model,\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Usual torch predict function. Supports sigmoid and softmax activations.\n",
    "    Args:\n",
    "        model (torch model): Model to predict with.\n",
    "        dataset (PathologyDataset): Dataset to predict on.\n",
    "        batch_size (int, optional): Batch size. Defaults to 64.\n",
    "        device (str, optional): Device for torch. Defaults to \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        numpy array [len(dataset) x num_classes]: Predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            pred = model(data['x_data'].to(device)).squeeze(-1)\n",
    "            pred = np.array([np.argwhere(x == x.max())for x in pred.detach().cpu().numpy()]).squeeze()\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = np.concatenate(preds, 0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainit(config, df_train, df_val, df_test, fold):\n",
    "    \"\"\"\n",
    "    Trains and validate a model.\n",
    "\n",
    "    Args:\n",
    "        config (Config): Parameters.\n",
    "        df_train (pandas dataframe): Training metadata.\n",
    "        df_val (pandas dataframe): Validation metadata.\n",
    "        df_test (pandas dataframe): Test metadata.\n",
    "        fold (int): Selected fold.\n",
    "\n",
    "    Returns:\n",
    "        np array: Study validation predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    # seed_everything(config.seed)\n",
    "\n",
    "    model = MlpModel(\n",
    "        input_dim=config.input_dim,\n",
    "        dense_dim=config.dense_dim,\n",
    "        num_classes=config.num_classes,\n",
    "        sep_dim1=config.sep_dim1,\n",
    "        sep_dim2=config.sep_dim2,        \n",
    "        comb_dim1=config.comb_dim1,\n",
    "        comb_dim2=config.comb_dim2,\n",
    "        last_dim=config.last_dim,\n",
    "    ).to(config.device)\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    train_dataset = BusinessData(df_train)\n",
    "    val_dataset = BusinessData(df_val)\n",
    "    test_dataset = BusinessData(df_test)\n",
    "\n",
    "    # n_parameters = count_parameters(model)\n",
    "\n",
    "    print(f\"    -> {len(train_dataset)} training \")\n",
    "    print(f\"    -> {len(val_dataset)} validation \")\n",
    "    # print(f\"    -> {n_parameters} trainable parameters\\n\")\n",
    "\n",
    "    pred_val = fit(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        loss_name=config.loss,\n",
    "        optimizer=config.optimizer,\n",
    "        epochs=config.epochs,\n",
    "        batch_size=config.batch_size,\n",
    "        val_bs=config.val_bs,\n",
    "        lr=config.lr,\n",
    "        warmup_prop=config.warmup_prop,\n",
    "        verbose=config.verbose,\n",
    "        first_epoch_eval=config.first_epoch_eval,\n",
    "        device=config.device,\n",
    "        entropy_wieght=config.entropy_weight\n",
    "    )\n",
    "    \n",
    "    pred_test = predict(\n",
    "        model, \n",
    "        test_dataset, \n",
    "        batch_size=config.val_bs, \n",
    "        device=config.device\n",
    "    )\n",
    "\n",
    "    # if config.save_weights:\n",
    "    #     save_model_weights(\n",
    "    #         model,\n",
    "    #         f\"{config.selected_model}_{fold}.pt\",\n",
    "    #         cp_folder=\"\",\n",
    "    #     )\n",
    "\n",
    "    del (model, train_dataset, val_dataset, test_dataset)\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return pred_val, pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def k_fold(config, df, df_test):\n",
    "    \"\"\"\n",
    "    Performs a patient grouped k-fold cross validation.\n",
    "    \"\"\"\n",
    "\n",
    "    pred_oof = np.zeros(len(df))\n",
    "    preds_test = []\n",
    "    \n",
    "    gkf = KFold(n_splits=config.k)\n",
    "    splits = list(gkf.split(X=df, y=df))\n",
    "\n",
    "    for i, (train_idx, val_idx) in enumerate(splits):\n",
    "        if i in config.selected_folds:\n",
    "            print(f\"\\n-------------   Fold {i + 1} / {config.k}  -------------\\n\")\n",
    "\n",
    "            df_train = df.iloc[train_idx].copy().reset_index(drop=True)\n",
    "            df_val = df.iloc[val_idx].copy().reset_index(drop=True)\n",
    "\n",
    "            pred_val, pred_test = trainit(config, df_train, df_val, df_test, i)\n",
    "            sns.displot(pred_val)\n",
    "            pred_oof[val_idx] = pred_val.flatten()\n",
    "            preds_test.append(pred_test.flatten())\n",
    "\n",
    "    print(f'\\n -> CV MAE : {compute_metric(df, pred_oof) :.3f}')\n",
    "\n",
    "    return pred_oof, np.mean(preds_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Parameters used for training\n",
    "    \"\"\"\n",
    "    # General\n",
    "    seed = 42\n",
    "    verbose = 1\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    save_weights = True\n",
    "\n",
    "    # k-fold\n",
    "    k = 4\n",
    "    selected_folds = [0,1,2,3]\n",
    "    \n",
    "    # Model\n",
    "\n",
    "    input_dim=INPUT_DIM\n",
    "    dense_dim=256\n",
    "    sep_dim1=128\n",
    "    sep_dim2=256       \n",
    "    comb_dim1=1024\n",
    "    comb_dim2=512\n",
    "    last_dim=256\n",
    "    num_classes=4\n",
    "    \n",
    "    # Training\n",
    "    loss = \"CrossEntropyLoss\"  # not used\n",
    "    entropy_weight = ENTROPY_WEIGHT\n",
    "    optimizer = \"Adam\"\n",
    "    batch_size = 8\n",
    "    epochs = 50\n",
    "\n",
    "    lr = 1e-3\n",
    "    warmup_prop = 0\n",
    "\n",
    "    val_bs = 256\n",
    "    first_epoch_eval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------   Fold 1 / 4  -------------\n",
      "\n",
      "    -> 60000 training \n",
      "    -> 20000 validation \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SEELEA~1\\AppData\\Local\\Temp/ipykernel_17968/2133151480.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"x_data\":  torch.tensor(self.x_data[index],dtype=torch.float),\n",
      "C:\\Users\\SEELEA~1\\AppData\\Local\\Temp/ipykernel_17968/2133151480.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"y_data\": torch.tensor(self.y_data[index],dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/50 \t lr=9.8e-04\t t=30s \tloss=1.281\tval_loss=1.295\taccuray=0.574\n",
      "Epoch 02/50 \t lr=9.6e-04\t t=29s \tloss=1.278\tval_loss=1.283\taccuray=0.574\n",
      "Epoch 03/50 \t lr=9.4e-04\t t=29s \tloss=1.278\tval_loss=1.286\taccuray=0.575\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SEELEA~1\\AppData\\Local\\Temp/ipykernel_17968/1286125532.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m pred_oof, pred_test = k_fold(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mConfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;32mC:\\Users\\SEELEA~1\\AppData\\Local\\Temp/ipykernel_17968/70302867.py\u001b[0m in \u001b[0;36mk_fold\u001b[1;34m(config, df, df_test)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mdf_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mpred_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mpred_oof\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SEELEA~1\\AppData\\Local\\Temp/ipykernel_17968/3582222953.py\u001b[0m in \u001b[0;36mtrainit\u001b[1;34m(config, df_train, df_val, df_test, fold)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# print(f\"    -> {n_parameters} trainable parameters\\n\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     pred_val = fit(\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SEELEA~1\\AppData\\Local\\Temp/ipykernel_17968/1769256943.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(model, train_dataset, val_dataset, loss_name, optimizer, epochs, batch_size, val_bs, warmup_prop, lr, num_classes, verbose, first_epoch_eval, device, entropy_wieght)\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             )\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\envs\\python38\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\envs\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programs\\envs\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_oof, pred_test = k_fold(\n",
    "    Config, \n",
    "    train,\n",
    "    test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sub['pressure'] = pred_test\n",
    "train[\"pred\"] = pred_oof\n",
    "train.to_csv('torch_train_pred.csv', index=False)\n",
    "# sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.0\n",
       "1        1.0\n",
       "2        1.0\n",
       "3        1.0\n",
       "4        1.0\n",
       "        ... \n",
       "19995    1.0\n",
       "19996    2.0\n",
       "19997    2.0\n",
       "19998    2.0\n",
       "19999    1.0\n",
       "Name: pred, Length: 20000, dtype: float64"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x20098e1c6d0>"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(train.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        2\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "19995    1\n",
       "19996    2\n",
       "19997    0\n",
       "19998    3\n",
       "19999    1\n",
       "Name: target, Length: 20000, dtype: int64"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5366"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train.target==train.pred).sum()/len(train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87bde4b8ac899a866791947b85fa526e720fe9897752d0655688f577c9f6fa63"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('python38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
